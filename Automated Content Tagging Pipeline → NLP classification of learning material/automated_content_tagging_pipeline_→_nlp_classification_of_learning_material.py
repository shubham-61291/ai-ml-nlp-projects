# -*- coding: utf-8 -*-
"""Automated Content Tagging Pipeline â†’ NLP classification of learning material.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rn_FrXz7JX-BZfueViYmYsS09ShViHEx
"""

# STEP 1: imports + load data
import pandas as pd

topics = pd.read_csv("/content/topics.csv")
tags   = pd.read_csv("/content/topics_with_tags.csv")

print("âœ… Loaded")
print("topics shape:", topics.shape)
print("tags shape  :", tags.shape)

# quick peek
display(topics.head())
display(tags.head())

# basic schema checks
need_topics = {"topic_id","title","description"}
need_tags   = {"topic_id","subject","difficulty","tag"}
print("\nMissing in topics:", list(need_topics - set(topics.columns)))
print("Missing in tags  :", list(need_tags   - set(tags.columns)))

# standardize base difficulty (objective, not per-student)
difficulty_map = {"Easy":1, "Medium":2, "Hard":3}
tags["difficulty_level"] = tags["difficulty"].map(difficulty_map)

# sanity: duplicates / unmatched ids
dupe_topics = topics["topic_id"].duplicated().sum()
dupe_tags   = tags["topic_id"].duplicated().sum()
print(f"\nDuplicates â†’ topics:{dupe_topics} | tags:{dupe_tags}")

unmatched_in_tags   = set(tags["topic_id"])   - set(topics["topic_id"])
unmatched_in_topics = set(topics["topic_id"]) - set(tags["topic_id"])
print("Unmatched topic_ids (in tags but not topics):", sorted(list(unmatched_in_tags))[:10])
print("Unmatched topic_ids (in topics but not tags):", sorted(list(unmatched_in_topics))[:10])

# show label distributions
print("\nSubject distribution:")
print(tags["subject"].value_counts())

print("\nDifficulty distribution (raw):")
print(tags["difficulty"].value_counts())

print("\nDifficulty distribution (coded):")
print(tags["difficulty_level"].value_counts().sort_index())

# STEP 2: Build supervised dataset for tagging

# combine text fields into one
topics["text"] = topics["title"] + " " + topics["description"]

# merge with tags to align inputs and labels
df = topics.merge(tags, on="topic_id", how="inner")

print("âœ… Supervised dataset ready")
print("Shape:", df.shape)

# peek
df[["topic_id", "text", "subject", "difficulty", "difficulty_level", "tag"]].head()

# encode target (subject classification for now)
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df["subject_label"] = le.fit_transform(df["subject"])

print("\nLabel Mapping (subject â†’ code):")
for s, c in zip(le.classes_, le.transform(le.classes_)):
    print(f"{s:12s} â†’ {c}")

print("Class distribution before splitting:")
print(df["subject"].value_counts())

# STEP 3: Train/Val/Test split + baseline classifier (with balanced dataset)

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# split dataset (80% train, 10% val, 10% test) with stratification
X_train, X_temp, y_train, y_temp = train_test_split(
    df["text"], df["subject_label"],
    test_size=0.2, random_state=42, stratify=df["subject_label"]
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.5, random_state=42, stratify=y_temp
)

print("Train size:", len(X_train), "Val size:", len(X_val), "Test size:", len(X_test))

# vectorize text using TF-IDF
tfidf = TfidfVectorizer(stop_words="english", max_features=1000)
X_train_tfidf = tfidf.fit_transform(X_train)
X_val_tfidf   = tfidf.transform(X_val)
X_test_tfidf  = tfidf.transform(X_test)

# train Logistic Regression classifier
clf = LogisticRegression(max_iter=1000, random_state=42)
clf.fit(X_train_tfidf, y_train)

# validation results
y_val_pred = clf.predict(X_val_tfidf)
print("\nðŸ“Š Validation Report")
print(classification_report(y_val, y_val_pred, target_names=le.classes_))

# test results
y_test_pred = clf.predict(X_test_tfidf)
print("\nâœ… Test Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nðŸ“Š Test Report")
print(classification_report(y_test, y_test_pred, target_names=le.classes_))

# plot confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - Baseline TF-IDF Model")
plt.show()

# STEP 4 â€” SUBJECT tagger with DistilBERT (manual PyTorch loop, no tokens, no HF/W&B logging)

!pip -q install -U transformers datasets accelerate scikit-learn

import os, math, numpy as np, pandas as pd, torch, itertools, matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score

from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    DataCollatorWithPadding, get_linear_schedule_with_warmup
)

# ---------- 4.1 Build dataset from your DFS ----------
df = topics.merge(tags, on="topic_id", how="inner").copy()
df["text"] = df["title"] + " " + df["description"]

# Encode SUBJECT labels
le = LabelEncoder().fit(df["subject"])
df["subject_label"] = le.transform(df["subject"])
id2label = {i: lbl for i, lbl in enumerate(le.classes_)}
label2id = {lbl: i for i, lbl in id2label.items()}
num_labels = len(le.classes_)
print("Subjects:", list(le.classes_))

# Stratified 80/10/10 split
X_train, X_temp, y_train, y_temp = train_test_split(
    df["text"], df["subject_label"], test_size=0.2, random_state=42, stratify=df["subject_label"]
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

def make_df(X, y): return pd.DataFrame({"text": X.values, "label": y.values})
train_df, val_df, test_df = make_df(X_train, y_train), make_df(X_val, y_val), make_df(X_test, y_test)

# Wrap with HF datasets (for easy tokenization)
ds = DatasetDict({
    "train": Dataset.from_pandas(train_df, preserve_index=False),
    "validation": Dataset.from_pandas(val_df, preserve_index=False),
    "test": Dataset.from_pandas(test_df, preserve_index=False),
})

# ---------- 4.2 Tokenizer & collator ----------
model_name = "distilbert-base-uncased"  # public, no token needed
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tok_fn(batch):
    return tokenizer(batch["text"], truncation=True, max_length=128)

ds_tok = ds.map(tok_fn, batched=True, remove_columns=["text"])
collator = DataCollatorWithPadding(tokenizer=tokenizer)

# set format to PyTorch tensors
cols = ["input_ids", "attention_mask", "label"]
for split in ds_tok.keys():
    ds_tok[split].set_format(type="torch", columns=cols)

# DataLoaders
from torch.utils.data import DataLoader
train_loader = DataLoader(ds_tok["train"], batch_size=8, shuffle=True, collate_fn=collator)
val_loader   = DataLoader(ds_tok["validation"], batch_size=16, shuffle=False, collate_fn=collator)
test_loader  = DataLoader(ds_tok["test"], batch_size=16, shuffle=False, collate_fn=collator)

# ---------- 4.3 Model ----------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, num_labels=num_labels, id2label=id2label, label2id=label2id
).to(device)

# Optimizer & scheduler
from torch.optim import AdamW
EPOCHS = 4
optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
num_training_steps = EPOCHS * len(train_loader)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

# ---------- 4.4 Train loop (manual) ----------
train_losses, val_losses = [], []

def run_epoch(dataloader, train_mode=True):
    if train_mode:
        model.train()
    else:
        model.eval()
    epoch_loss = 0.0
    n_batches = 0
    all_preds, all_labels = [], []

    for batch in dataloader:
        # Move to device
        batch = {k: v.to(device) for k, v in batch.items()}

        if train_mode:
            optimizer.zero_grad()

        # Forward
        outputs = model(input_ids=batch["input_ids"],
                        attention_mask=batch["attention_mask"],
                        labels=batch["labels"])
        loss = outputs.loss
        logits = outputs.logits

        if train_mode:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()

        epoch_loss += loss.item()
        n_batches += 1

        preds = torch.argmax(logits, dim=-1).detach().cpu().numpy().tolist()
        labels = batch["labels"].detach().cpu().numpy().tolist()
        all_preds.extend(preds)
        all_labels.extend(labels)

    avg_loss = epoch_loss / max(1, n_batches)
    acc = accuracy_score(all_labels, all_preds)
    f1m = f1_score(all_labels, all_preds, average="macro")
    return avg_loss, acc, f1m

for epoch in range(1, EPOCHS + 1):
    print(f"\nðŸš€ Epoch {epoch}/{EPOCHS}")

    tr_loss, tr_acc, tr_f1 = run_epoch(train_loader, train_mode=True)
    print(f"Train   â†’ loss: {tr_loss:.4f} | acc: {tr_acc:.4f} | f1_macro: {tr_f1:.4f}")

    val_loss, val_acc, val_f1 = run_epoch(val_loader, train_mode=False)
    print(f"Valid   â†’ loss: {val_loss:.4f} | acc: {val_acc:.4f} | f1_macro: {val_f1:.4f}")

    train_losses.append(tr_loss)
    val_losses.append(val_loss)

# ---------- 4.5 Plot loss curves ----------
plt.figure(figsize=(8,6))
plt.plot(range(1, len(train_losses)+1), train_losses, marker="o", label="Train Loss")
plt.plot(range(1, len(val_losses)+1), val_losses, marker="o", label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss (DistilBERT - manual loop)")
plt.legend()
plt.grid(True)
plt.show()

# ---------- 4.6 Test evaluation ----------
model.eval()
all_preds, all_labels = [], []
with torch.no_grad():
    for batch in test_loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        out = model(input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    labels=batch["labels"])
        logits = out.logits
        preds = torch.argmax(logits, dim=-1).detach().cpu().numpy().tolist()
        labels = batch["labels"].detach().cpu().numpy().tolist()
        all_preds.extend(preds)
        all_labels.extend(labels)

print("\nâœ… Test Accuracy:", accuracy_score(all_labels, all_preds))
print("\nðŸ“Š Classification Report (Test)")
print(classification_report(all_labels, all_preds, target_names=[id2label[i] for i in range(num_labels)]))

# Confusion matrix
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(6,5))
plt.imshow(cm, cmap="Blues")
plt.title("DistilBERT â€” Confusion Matrix (Test)")
plt.colorbar()
ticks = np.arange(num_labels)
plt.xticks(ticks, [id2label[i] for i in ticks], rotation=45)
plt.yticks(ticks, [id2label[i] for i in ticks])
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, cm[i, j], ha="center", va="center")
plt.xlabel("Predicted"); plt.ylabel("True")
plt.tight_layout()
plt.show()

# ---------- 4.7 (Optional) Export predictions for all topics ----------
def predict_subject(texts):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device).eval()
    all_labels = []
    for i in range(0, len(texts), 64):
        batch_texts = texts[i:i+64]
        enc = tokenizer(batch_texts, return_tensors="pt", truncation=True, padding=True, max_length=128).to(device)
        with torch.no_grad():
            probs = torch.softmax(model(**enc).logits, dim=-1)
        idxs = probs.argmax(dim=-1).cpu().numpy().tolist()
        all_labels.extend([id2label[i] for i in idxs])
    return all_labels

all_texts = (topics["title"] + " " + topics["description"]).tolist()
pred_labels = predict_subject(all_texts)
pred_df = topics.copy()
pred_df["subject_pred"] = pred_labels
pred_df.to_csv("topics_predicted_subjects.csv", index=False)
print("ðŸ’¾ Saved:", os.path.abspath("topics_predicted_subjects.csv"))