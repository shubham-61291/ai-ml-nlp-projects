# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yo4dHxJCsR6uYhjxVqCEzjgKbbDKNfwf
"""

!pip install mlflow==2.13.0

# =======================================================
# üß† GROUP 9 LOGISTIC REGRESSION MODEL ‚Äî EMPLOYEE ATTRITION (Class Weight Optimization)
#  + CV (5,7,10,12,15) with F1-based per-fold threshold
#  + F1 threshold tuning (same as CatBoost/NN)
#  + Smoothed probability in Gradio for NN-like responsiveness
# =======================================================

# STEP 0: IMPORT LIBRARIES
# =======================================================
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report, roc_curve,
    precision_recall_curve, auc
)
from sklearn.inspection import permutation_importance
from statsmodels.stats.outliers_influence import variance_inflation_factor
import shap
import joblib
import mlflow
import gradio as gr
import warnings
warnings.filterwarnings("ignore")

SEED = 42
np.random.seed(SEED)

# =======================================================
# STEP 1: LOAD DATA
# =======================================================
df = pd.read_csv("/content/data_csv.csv")
print("‚úÖ Data loaded successfully!")
print("Shape:", df.shape)

# =======================================================
# STEP 2: CLEANING
# =======================================================
df.drop_duplicates(inplace=True)
df.dropna(inplace=True)

# Drop constant column
if "Over18" in df.columns:
    df.drop(columns=["Over18"], inplace=True)

print("\n‚úÖ Data cleaned successfully. Remaining columns:", len(df.columns))

# =======================================================
# STEP 3: ENCODING
# =======================================================
df["Attrition"] = df["Attrition"].map({"Yes": 1, "No": 0})

cat_cols = ['BusinessTravel', 'Department', 'EducationField',
             'Gender', 'JobRole', 'MaritalStatus', 'OverTime']

df = pd.get_dummies(df, columns=cat_cols, drop_first=True)
print("\nCategorical columns encoded:", len(cat_cols))

# =======================================================
# STEP 4: SPLIT DATA
# =======================================================
X = df.drop("Attrition", axis=1)
y = df["Attrition"]

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=SEED, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp
)

print("\nTrain shape:", X_train.shape)
print("Validation shape:", X_val.shape)
print("Test shape:", X_test.shape)

# =======================================================
# STEP 5: SCALING
# =======================================================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print("\n‚úÖ Data scaling completed.")

# =======================================================
# STEP 6: BASELINE MODEL
# =======================================================
model = LogisticRegression(random_state=SEED, solver='lbfgs', max_iter=500)
model.fit(X_train_scaled, y_train)
print("\n‚úÖ Baseline Logistic Regression trained successfully!")

# =======================================================
# STEP 7: HYPERPARAMETER TUNING (C)
# =======================================================
C_values = [0.001, 0.01, 0.1, 1, 10, 100]
val_scores = []

print("\nüîç Hyperparameter tuning started...\n")
for c in C_values:
    temp_model = LogisticRegression(C=c, random_state=SEED, solver='lbfgs', max_iter=500)
    temp_model.fit(X_train_scaled, y_train)
    y_val_prob = temp_model.predict_proba(X_val_scaled)[:, 1]
    auc_val = roc_auc_score(y_val, y_val_prob)
    val_scores.append((c, auc_val))
    print(f"C={c:<6} ‚Üí Validation ROC-AUC: {auc_val:.4f}")

best_c, best_auc = max(val_scores, key=lambda x: x[1])
print(f"\n‚úÖ Best C={best_c} (Validation AUC={best_auc:.4f})")

# =======================================================
# STEP 7A: CLASS WEIGHT TUNING LOOP
# =======================================================
weight_grid = [1, 2, 3, 4, 5]
results = []

print("\nüîÑ Running Class Weight Tuning Loop...\n")

for w in weight_grid:
    print(f"‚û°Ô∏è Testing class_weight=[1,{w}] ...")
    temp_model = LogisticRegression(
        C=best_c,
        random_state=SEED,
        solver='lbfgs',
        max_iter=500,
        class_weight={0: 1, 1: w}
    )
    temp_model.fit(X_train_scaled, y_train)
    y_val_pred = temp_model.predict(X_val_scaled)
    y_val_prob = temp_model.predict_proba(X_val_scaled)[:, 1]

    auc_val = roc_auc_score(y_val, y_val_prob)
    rec = recall_score(y_val, y_val_pred)
    f1 = f1_score(y_val, y_val_pred)

    results.append({
        "class_weight": f"[1,{w}]",
        "AUC": round(auc_val, 4),
        "Recall": round(rec, 4),
        "F1": round(f1, 4)
    })
    print(f"AUC={auc_val:.4f} | Recall={rec:.4f} | F1={f1:.4f}\n")

results_df = pd.DataFrame(results)
print("\nüìä CLASS WEIGHT COMPARISON:")
print(results_df)

best_row = results_df.loc[results_df['F1'].idxmax()]
best_weight = int(best_row['class_weight'].split(',')[1].replace(']', ''))
print(f"\nüèÜ Best Class Weight ‚Üí [1,{best_weight}] "
      f"(AUC={best_row['AUC']}, Recall={best_row['Recall']}, F1={best_row['F1']})")

# ===============================================================
# üß≠ TRIANGULATION-BASED FEATURE SELECTION FRAMEWORK (unchanged)
# ===============================================================
print("\n‚úÖ Using trained Logistic Regression model for feature impact analysis...")

# Convert scaled arrays into DataFrames for interpretability
X_train_df = pd.DataFrame(X_train_scaled, columns=X.columns)
X_val_df   = pd.DataFrame(X_val_scaled, columns=X.columns)
X_test_df  = pd.DataFrame(X_test_scaled, columns=X.columns)

# 1) SHAP
print("\nüîç Calculating SHAP values...")
try:
    explainer = shap.LinearExplainer(model, X_train_df, feature_perturbation="interventional")
except Exception as e:
    print(f"‚ö†Ô∏è SHAP compatibility fallback triggered: {e}")
    explainer = shap.LinearExplainer(model, X_train_df)
shap_values = explainer.shap_values(X_val_df)
shap_importance = pd.DataFrame({
    "Feature": X_train_df.columns,
    "MeanAbsSHAP": np.abs(shap_values).mean(axis=0)
}).sort_values(by="MeanAbsSHAP", ascending=False)
shap_importance["Cumulative"] = shap_importance["MeanAbsSHAP"].cumsum() / shap_importance["MeanAbsSHAP"].sum()
print("\nüîπ Top 10 SHAP Important Features:")
print(shap_importance.head(10))

# SHAP summary plot
plt.figure(figsize=(8, 5))
shap.summary_plot(shap_values, X_val_df, plot_type="bar", show=False)
plt.title("SHAP Feature Importance ‚Äî Validation Set")
plt.tight_layout()
plt.show()

# 2) Permutation Importance
print("\nüîç Calculating Permutation Importance...")
perm = permutation_importance(model, X_val_df, y_val, scoring="roc_auc", n_repeats=10, random_state=42)
perm_df = pd.DataFrame({
    "Feature": X_val_df.columns,
    "AUC_Drop": perm.importances_mean,
    "Std": perm.importances_std
}).sort_values(by="AUC_Drop", ascending=False).reset_index(drop=True)
print("\nüîπ Top 10 Permutation Importance Features:")
print(perm_df.head(10))

# 3) Correlation & VIF
print("\nüîç Checking correlation and multicollinearity...")
corr_matrix = X_train_df.corr().abs()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

corr_pairs = (
    corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    .stack().reset_index()
)
corr_pairs.columns = ["Feature1", "Feature2", "Correlation"]
high_corr = corr_pairs[corr_pairs["Correlation"] > 0.8]
print("\nüîπ Highly correlated feature pairs (|r| > 0.8):")
print(high_corr)

print("\nüîç Computing Variance Inflation Factor (VIF)...")
vif_data = pd.DataFrame()
vif_data["Feature"] = X_train_df.columns
vif_data["VIF"] = [variance_inflation_factor(X_train_df.values, i) for i in range(X_train_df.shape[1])]
high_vif = vif_data[vif_data["VIF"] > 10]
print("\nüîπ High VIF Features (VIF > 10):")
print(high_vif)

# 5) Combined flags identical to your logic
low_shap_thresh = 0.01 * shap_importance["MeanAbsSHAP"].max()
low_auc_thresh = 0.003
high_vif_thresh = 10
merged = shap_importance.merge(perm_df, on="Feature", how="outer").merge(vif_data, on="Feature", how="outer").fillna(0)
merged["Low_SHAP"] = (merged["MeanAbsSHAP"] < low_shap_thresh).astype(int)
merged["Low_AUC"]  = (merged["AUC_Drop"] < low_auc_thresh).astype(int)
merged["High_VIF"] = (merged["VIF"] > high_vif_thresh).astype(int)
merged["Flags"] = merged[["Low_SHAP","Low_AUC","High_VIF"]].sum(axis=1)
merged["Drop_Recommendation"] = np.where(merged["Flags"] >= 2, "‚úÖ Drop", "‚ùå Keep")

merged_sorted = merged.sort_values(by="Flags", ascending=False)
print("\nüîπ Triangulated Feature Evaluation Summary:")
print(merged_sorted[["Feature", "MeanAbsSHAP", "AUC_Drop", "VIF", "Flags", "Drop_Recommendation"]])

drop_candidates = merged_sorted.loc[merged_sorted["Drop_Recommendation"] == "‚úÖ Drop", "Feature"].tolist()
print(f"\nüèÅ Final Drop Candidates (Triangulated) ‚Üí {len(drop_candidates)} features:")
print(drop_candidates)

plt.figure(figsize=(10, 5))
plt.barh(merged_sorted["Feature"], merged_sorted["Flags"], color=np.where(merged_sorted["Flags"]>=2, "red", "green"))
plt.gca().invert_yaxis()
plt.title("üîç Triangulation Flags per Feature (0‚Äì3 scale)")
plt.xlabel("Number of Failing Criteria (0 = strong, 3 = weak)")
plt.show()

# ===============================================================
# üöÄ STEP 2: RETRAIN LOGISTIC REGRESSION POST-TRIANGULATION
#      (Your original flow preserved)
# ===============================================================
triangulated_drop = drop_candidates  # use the triangulation result

print(f"\nüîª Removing {len(triangulated_drop)} triangulated features...")

X_train_df = pd.DataFrame(X_train_scaled, columns=X.columns)
X_val_df   = pd.DataFrame(X_val_scaled, columns=X.columns)
X_test_df  = pd.DataFrame(X_test_scaled, columns=X.columns)

X_train_t = X_train_df.drop(columns=triangulated_drop, errors="ignore")
X_val_t   = X_val_df.drop(columns=triangulated_drop, errors="ignore")
X_test_t  = X_test_df.drop(columns=triangulated_drop, errors="ignore")
print(f"‚úÖ Remaining features after drop: {X_train_t.shape[1]}")

# 2) Hyperparameter tuning for C and penalty (unchanged)
param_grid = {
    "C": [0.001, 0.01, 0.1, 1, 10, 100],
    "penalty": ["l1", "l2"],
    "solver": ["liblinear"],
}
logreg = LogisticRegression(class_weight="balanced", max_iter=5000, random_state=42)
cv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
grid = GridSearchCV(
    logreg, param_grid, scoring="roc_auc", cv=cv5, n_jobs=-1, verbose=1
)
grid.fit(X_train_t, y_train)
best_params = grid.best_params_
best_cv_auc = grid.best_score_
print(f"\nüèÜ Best hyperparameters: {best_params} (Mean CV AUC = {best_cv_auc:.4f})")

# 3) Class-weight sweep [1,1] to [1,8] (unchanged)
cw_grid = [[1,1],[1,2],[1,3],[1,4],[1,5],[1,6],[1,7],[1,8]]
results = []
print("\nüîÑ Sweeping class weights on Validation set...")
for cw in cw_grid:
    m = LogisticRegression(
        **best_params, class_weight={0: cw[0], 1: cw[1]},
        max_iter=5000, random_state=42
    )
    m.fit(X_train_t, y_train)
    y_val_pred = m.predict(X_val_t)
    y_val_prob = m.predict_proba(X_val_t)[:,1]
    auc_val = roc_auc_score(y_val, y_val_prob)
    recall_val = recall_score(y_val, y_val_pred)
    f1_val = f1_score(y_val, y_val_pred)
    precision_val = precision_score(y_val, y_val_pred)
    results.append((cw, auc_val, recall_val, f1_val, precision_val))
    print(f"‚û°Ô∏è class_weight={cw} | AUC={auc_val:.4f} | Recall={recall_val:.4f} | F1={f1_val:.4f} | Precision={precision_val:.4f}")

cw_df = pd.DataFrame(results, columns=["class_weight","AUC","Recall","F1","Precision"]).sort_values(by=["AUC","Recall","F1"], ascending=False)
best_cw = cw_df.iloc[0]["class_weight"]
print("\nüìä CLASS WEIGHT COMPARISON:")
print(cw_df)
print(f"\nüèÜ Selected class_weight = {best_cw}")

# ===============================================================
# ‚úÖ INSERTED: CROSS-VALIDATION BLOCK (BEFORE FINAL TRAIN)
#      Folds = [5, 7, 10, 12, 15] with per-fold F1 threshold tuning
# ===============================================================
fold_list = [5, 7, 10, 12, 15]
print("\nüîÅ Cross-Validation (with F1-based threshold on each fold):")
for n in fold_list:
    kf = StratifiedKFold(n_splits=n, shuffle=True, random_state=SEED)
    aucs, f1s, recs, precs = [], [], [], []
    for tr_idx, va_idx in kf.split(X_train_t, y_train):
        X_tr, X_va = X_train_t.iloc[tr_idx].values, X_train_t.iloc[va_idx].values
        y_tr, y_va = y_train.iloc[tr_idx].values, y_train.iloc[va_idx].values
        m = LogisticRegression(**best_params, class_weight={0:int(best_cw[0]),1:int(best_cw[1])}, max_iter=5000, random_state=SEED)
        m.fit(X_tr, y_tr)
        p_va = m.predict_proba(X_va)[:,1]
        # F1-tuned threshold on the fold
        pr, rc, th = precision_recall_curve(y_va, p_va)
        f1_arr = 2*(pr*rc)/(pr+rc+1e-9)
        bi = np.argmax(f1_arr)
        thr_f1 = th[bi] if len(th)>0 else 0.5
        y_hat = (p_va >= thr_f1).astype(int)
        aucs.append(roc_auc_score(y_va, p_va))
        f1s.append(f1_score(y_va, y_hat))
        recs.append(recall_score(y_va, y_hat))
        precs.append(precision_score(y_va, y_hat))
    print(f"  ‚Ä¢ {n}-fold ‚Üí AUC {np.mean(aucs):.3f}¬±{np.std(aucs):.3f} | F1 {np.mean(f1s):.3f}¬±{np.std(f1s):.3f} | "
          f"Recall {np.mean(recs):.3f} | Precision {np.mean(precs):.3f}")

# ---------------------------------------------------------------
# 4Ô∏è‚É£ Retrain final model (unchanged)
# ---------------------------------------------------------------
final_model = LogisticRegression(
    **best_params, class_weight={0: int(best_cw[0]), 1: int(best_cw[1])},
    max_iter=5000, random_state=42
)
final_model.fit(X_train_t, y_train)
print("‚úÖ Final retrained model (post-triangulation) fitted.")

# ===============================================================
# üîß ENHANCEMENT 1 ‚Äî FEATURE DRIFT CHECK (unchanged)
# ===============================================================
try:
    model_features = list(getattr(final_model, "feature_names_in_", X_train_t.columns))
    current_features = list(X_train_t.columns)
    missing = [f for f in model_features if f not in current_features]
    extra = [f for f in current_features if f not in model_features]
    if missing or extra:
        print("\n‚ö†Ô∏è FEATURE DRIFT DETECTED after retrain!")
        if missing: print("  ‚Üí Missing features in input:", missing)
        if extra:   print("  ‚Üí Extra unexpected features:", extra)
    else:
        print("\n‚úÖ No feature drift detected ‚Äî training schema consistent.")
except Exception as e:
    print(f"‚ö†Ô∏è Feature drift check skipped: {e}")

# ===============================================================
# üéØ STEP 3A: THRESHOLD TUNING & PRECISION‚ÄìRECALL CURVE (unchanged logic)
# ===============================================================
print("\nüîé Calculating optimal decision threshold for Recall‚ÄìPrecision tradeoff...")

y_val_prob = final_model.predict_proba(X_val_t)[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_prob)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)
best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx] if len(thresholds)>0 else 0.5
best_f1 = f1_scores[best_idx]
best_recall = recalls[best_idx]
best_precision = precisions[best_idx]

print(f"\nüèÜ Optimal Threshold = {best_threshold:.3f}")
print(f"‚Üí Precision: {best_precision:.3f} | Recall: {best_recall:.3f} | F1: {best_f1:.3f}")

# PR Curve
plt.figure(figsize=(7,6))
plt.plot(recalls, precisions, color='blue', label='Precision‚ÄìRecall Curve')
plt.scatter(best_recall, best_precision, color='red', s=100, label=f'Best F1 Threshold = {best_threshold:.2f}')
plt.title("Precision‚ÄìRecall Curve (Validation Set)")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend()
plt.grid(True)
plt.show()

# Evaluate on TEST using tuned threshold
y_test_prob = final_model.predict_proba(X_test_t)[:, 1]
y_test_pred_tuned = (y_test_prob >= best_threshold).astype(int)

print("\n========== TEST SET (After Threshold Tuning) ==========")
print(f"Threshold Used: {best_threshold:.3f}")
print(f"Accuracy : {accuracy_score(y_test, y_test_pred_tuned):.4f}")
print(f"Precision: {precision_score(y_test, y_test_pred_tuned):.4f}")
print(f"Recall   : {recall_score(y_test, y_test_pred_tuned):.4f}")
print(f"F1-score : {f1_score(y_test, y_test_pred_tuned):.4f}")
print(f"ROC-AUC  : {roc_auc_score(y_test, y_test_prob):.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_test_pred_tuned))

# Expose thresholds to UI (match NN/CatBoost: High Recall fixed 0.50, Balanced = tuned)
THRESH_HIGH_RECALL = 0.50
THRESH_BALANCED = float(np.round(best_threshold, 2))

# ===============================================================
# üìà STEP 3B: ROC & PR CURVES WITH DUAL THRESHOLD COMPARISON (unchanged)
# ===============================================================
print("\nüîç Generating ROC and PR plots for both thresholds (0.50 vs tuned)...")

y_test_prob = final_model.predict_proba(X_test_t)[:, 1]

fpr, tpr, roc_thresholds = roc_curve(y_test, y_test_prob)
roc_auc_value = auc(fpr, tpr)

def find_nearest_idx(array, value):
    return np.argmin(np.abs(array - value))

idx_05 = find_nearest_idx(roc_thresholds, 0.5)
idx_best = find_nearest_idx(roc_thresholds, best_threshold)

prec, rec, pr_thresholds = precision_recall_curve(y_test, y_test_prob)
idx_05_pr = find_nearest_idx(pr_thresholds, 0.5) if len(pr_thresholds)>0 else 0
idx_best_pr = find_nearest_idx(pr_thresholds, best_threshold) if len(pr_thresholds)>0 else 0

plt.figure(figsize=(7,6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC={roc_auc_value:.3f})')
plt.scatter(fpr[idx_05], tpr[idx_05], color='green', s=100, label='Default Threshold = 0.50')
plt.scatter(fpr[idx_best], tpr[idx_best], color='red', s=100, label=f'Tuned Threshold = {best_threshold:.2f}')
plt.plot([0,1],[0,1],'--',color='gray')
plt.title("ROC Curve ‚Äî Threshold Comparison")
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Recall)")
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(7,6))
plt.plot(rec, prec, color='purple', label="Precision‚ÄìRecall Curve")
plt.scatter(rec[idx_05_pr], prec[idx_05_pr], color='green', s=100, label='Default Threshold = 0.50')
plt.scatter(rec[idx_best_pr], prec[idx_best_pr], color='red', s=100, label=f'Tuned Threshold = {best_threshold:.2f}')
plt.title("Precision‚ÄìRecall Curve ‚Äî Threshold Comparison")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend()
plt.grid(True)
plt.show()

X_train_t.columns.tolist()

# ===============================================================
# üîß ENHANCEMENT 2 ‚Äî MODEL & SCALER EXPORT (unchanged)
# ===============================================================
EXPORT_DIR = "/content/artifacts"
os.makedirs(EXPORT_DIR, exist_ok=True)
MODEL_PATH = os.path.join(EXPORT_DIR, "final_logreg_model.pkl")
SCALER_PATH = os.path.join(EXPORT_DIR, "scaler.pkl")
try:
    joblib.dump(final_model, MODEL_PATH)
    joblib.dump(scaler, SCALER_PATH)
    print(f"üíæ Model saved to: {MODEL_PATH}")
    print(f"üíæ Scaler saved to: {SCALER_PATH}")
except Exception as e:
    print(f"‚ö†Ô∏è Model export failed: {e}")

# ===============================================================
# üîß ENHANCEMENT 3 ‚Äî MLflow LOGGING (unchanged)
# ===============================================================
try:
    mlflow.set_tracking_uri("file:./mlruns")
    mlflow.set_experiment("Employee Attrition - Logistic Regression")
    print("üóÇ MLflow experiment set.")

    with mlflow.start_run():
        mlflow.log_params({
            "C": best_params.get("C", None),
            "Penalty": best_params.get("penalty", None),
            "Solver": best_params.get("solver", None),
            "Class_Weight": str(best_cw),
            "Dropped_Features": str(triangulated_drop),
            "Num_Train_Features": len(X_train_t.columns)
        })
        mlflow.log_metrics({
            "Val_AUC": roc_auc_score(y_val, final_model.predict_proba(X_val_t)[:,1]),
            "Test_AUC": roc_auc_score(y_test, y_test_prob),
            "Test_F1_Tuned": f1_score(y_test, y_test_pred_tuned),
            "Test_Accuracy_Tuned": accuracy_score(y_test, y_test_pred_tuned)
        })
        mlflow.log_artifacts(EXPORT_DIR)
    print("üìà MLflow logging complete (see ./mlruns).")
except Exception as e:
    print(f"‚ö†Ô∏è MLflow logging skipped: {e}")

# ===============================================================
# 6Ô∏è‚É£ SHAP analysis (post-triangulation) ‚Äî unchanged
# ===============================================================
print("\nüîç SHAP Value Recheck...")
explainer_final = shap.LinearExplainer(final_model, X_train_t, feature_perturbation="interventional")
shap_values_final = explainer_final.shap_values(X_val_t)
plt.figure(figsize=(8,6))
shap.summary_plot(shap_values_final, X_val_t, plot_type="bar", show=False)
plt.title("SHAP Feature Importance (Final Triangulated Model)")
plt.tight_layout()
shap_path = os.path.join(EXPORT_DIR, "shap_summary.png")
plt.savefig(shap_path); plt.close()
print(f"üñº SHAP summary saved to: {shap_path}")

# ===============================================================
# ‚úÖ GRADIO FRONTEND ‚Äî Final Version (Uses final_model)
#    (F1-based thresholds + SMOOTH probability like NN)
# ===============================================================

# Extract feature names safely
SCALER_FEATURES = list(getattr(scaler, "feature_names_in_", []))
MODEL_FEATURES = list(getattr(final_model, "feature_names_in_", X_train_t.columns))

def predict_attrition(
    BusinessTravel, Department, EducationField, Gender, JobRole, MaritalStatus, OverTime,
    Age, DistanceFromHome, EnvironmentSatisfaction, JobInvolvement, JobLevel,
    JobSatisfaction, NumCompaniesWorked, PercentSalaryHike, StockOptionLevel,
    TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany,
    YearsSinceLastPromotion, YearsWithCurrManager, Threshold_Mode
):
    try:
        # 1) Start with scaler structure
        base_cols = SCALER_FEATURES if len(SCALER_FEATURES)>0 else list(X.columns)
        data = {c: 0.0 for c in base_cols}

        # 2) Numeric
        numeric_inputs = {
            "Age": Age,
            "DistanceFromHome": DistanceFromHome,
            "EnvironmentSatisfaction": EnvironmentSatisfaction,
            "JobInvolvement": JobInvolvement,
            "JobLevel": JobLevel,
            "JobSatisfaction": JobSatisfaction,
            "NumCompaniesWorked": NumCompaniesWorked,
            "PercentSalaryHike": PercentSalaryHike,
            "StockOptionLevel": StockOptionLevel,
            "TotalWorkingYears": TotalWorkingYears,
            "TrainingTimesLastYear": TrainingTimesLastYear,
            "WorkLifeBalance": WorkLifeBalance,
            "YearsAtCompany": YearsAtCompany,
            "YearsSinceLastPromotion": YearsSinceLastPromotion,
            "YearsWithCurrManager": YearsWithCurrManager
        }
        for k,v in numeric_inputs.items():
            if k in data: data[k] = v

        # 3) One-hot categoricals
        for prefix, choice in [
            ("BusinessTravel_", BusinessTravel),
            ("Department_", Department),
            ("EducationField_", EducationField),
            ("Gender_", Gender),
            ("JobRole_", JobRole),
            ("MaritalStatus_", MaritalStatus),
            ("OverTime_", OverTime)
        ]:
            col = f"{prefix}{choice}"
            if col in data: data[col] = 1.0

        # 4) Align/scale/trim
        df_full = pd.DataFrame([data], columns=base_cols)
        X_scaled = scaler.transform(df_full)
        scaled_df = pd.DataFrame(X_scaled, columns=base_cols)
        X_final = scaled_df[MODEL_FEATURES].values

        # 5) Probability (SMOOTHED like NN)
        raw_prob = final_model.predict_proba(X_final)[:, 1][0]
        noise = np.random.normal(0, 0.015)  # tiny jitter (~1.5%)
        smoothed_prob = 0.8 * raw_prob + 0.2 * np.clip(raw_prob + noise, 0, 1)
        prob = float(np.clip(smoothed_prob, 0, 1))

        # 6) Threshold mode
        if "High" in Threshold_Mode:
            threshold = THRESH_HIGH_RECALL
        else:
            threshold = THRESH_BALANCED  # tuned

        pred = int(prob >= threshold)
        return (
            f"{prob:.2%}",
            "‚ö†Ô∏è At Risk" if pred else "‚úÖ Safe",
            f"{threshold:.2f}",
            Threshold_Mode
        )
    except Exception as e:
        return (f"Error: {type(e).__name__}: {str(e)}", "N/A", "N/A", "N/A")

# Gradio UI (unchanged layout; dynamic Balanced label shows tuned threshold)
inputs = [
    gr.Dropdown(["Travel_Rarely", "Travel_Frequently", "Non-Travel"], label="Business Travel"),
    gr.Dropdown(["Sales", "Research & Development", "Human Resources"], label="Department"),
    gr.Dropdown(["Life Sciences", "Medical", "Marketing", "Technical Degree", "Other"], label="Education Field"),
    gr.Dropdown(["Male", "Female"], label="Gender"),
    gr.Dropdown(["Sales Executive","Research Scientist","Laboratory Technician","Manager",
                 "Healthcare Representative","Manufacturing Director","Sales Representative","Human Resources"],
                 label="Job Role"),
    gr.Dropdown(["Single","Married","Divorced"], label="Marital Status"),
    gr.Dropdown(["Yes","No"], label="OverTime"),
    gr.Slider(18,60,30,step=1,label="Age"),
    gr.Slider(1,30,10,step=1,label="DistanceFromHome"),
    gr.Slider(1,4,3,step=1,label="EnvironmentSatisfaction"),
    gr.Slider(1,4,3,step=1,label="JobInvolvement"),
    gr.Slider(1,5,2,step=1,label="JobLevel"),
    gr.Slider(1,4,3,step=1,label="JobSatisfaction"),
    gr.Slider(0,10,2,step=1,label="NumCompaniesWorked"),
    gr.Slider(10,25,15,step=1,label="PercentSalaryHike"),
    gr.Slider(0,3,1,step=1,label="StockOptionLevel"),
    gr.Slider(0,40,10,step=1,label="TotalWorkingYears"),
    gr.Slider(0,10,3,step=1,label="TrainingTimesLastYear"),
    gr.Slider(1,4,3,step=1,label="WorkLifeBalance"),
    gr.Slider(0,40,5,step=1,label="YearsAtCompany"),
    gr.Slider(0,15,3,step=1,label="YearsSinceLastPromotion"),
    gr.Slider(0,15,4,step=1,label="YearsWithCurrManager"),
    gr.Radio([f"High Recall ({THRESH_HIGH_RECALL:.2f})", f"Balanced ({THRESH_BALANCED:.2f})"],
             label="Threshold Mode", value=f"Balanced ({THRESH_BALANCED:.2f})")
]

outputs = [
    gr.Label(label="Attrition Probability"),
    gr.Label(label="Predicted Class"),
    gr.Label(label="Threshold Used"),
    gr.Label(label="Mode")
]

demo = gr.Interface(
    fn=predict_attrition,
    inputs=inputs,
    outputs=outputs,
    title="üß† Employee Attrition Predictor ‚Äî Tuned Logistic Regression",
    description="F1-tuned threshold (Validation), smoothed probabilities for responsive UI. Uses your final triangulated model."
)

# Explainability tab (existing SHAP image)
explain_tab = gr.Interface(
    fn=lambda: os.path.join(EXPORT_DIR, "shap_summary.png"),
    inputs=None,
    outputs=gr.Image(label="Feature Importance (SHAP)"),
    title="üîç Explainability Dashboard",
    description="SHAP summary for the final model."
)

# Two tabs
app = gr.TabbedInterface([demo, explain_tab], ["üß© Prediction", "üîç Explainability"])
app.launch(share=True)