# -*- coding: utf-8 -*-
"""MARS – Memory and Retention Scheduling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QSvFWIB42CC6OhbWe5a00l8xH4aead5O
"""

# STEP 2.1 — Load data & basic schema checks

import pandas as pd
import numpy as np

# 1) Read the three CSVs you uploaded to Colab
students = pd.read_csv("students.csv")
topics = pd.read_csv("topics.csv")
interactions = pd.read_csv("interactions.csv")

# 2) Ensure date types for interaction logs
for col in ["learned_on", "obs_date"]:
    interactions[col] = pd.to_datetime(interactions[col], errors="coerce")

# 3) Verify required columns exist
exp_students = {"student_id", "grade", "ability", "diligence"}
exp_topics   = {"topic_id", "subject", "difficulty"}
exp_inter    = {"student_id","topic_id","learned_on","obs_date","days_since_learned","score_observed"}

missing = {
    "students.csv": list(exp_students - set(students.columns)),
    "topics.csv":   list(exp_topics   - set(topics.columns)),
    "interactions.csv": list(exp_inter - set(interactions.columns)),
}
problems = {k:v for k,v in missing.items() if v}
if problems:
    raise ValueError(f"Missing required columns: {problems}")

# 4) Quick summary so you can eyeball things
print("✅ Loaded OK")
print("Shapes → students:", students.shape, "| topics:", topics.shape, "| interactions:", interactions.shape)
print("Unique → students:", interactions['student_id'].nunique(), "| topics:", interactions['topic_id'].nunique())
print("days_since_learned values:", sorted(interactions['days_since_learned'].unique()))
display(students.head())
display(topics.head())
display(interactions.head())

def estimate_lambda(sub_df, k_points=3):
    """
    Fit log-linear model using earliest k_points.
    Returns: (R0_hat, lambda_hat)
    """
    d = sub_df.nsmallest(k_points, "days_since_learned")
    r = np.clip(d["score_observed"].values/100.0, 1e-4, 1.0)  # normalize to 0–1
    t = d["days_since_learned"].values.astype(float)

    # need at least 2 distinct time points
    if len(np.unique(t)) < 2:
        return np.nan, np.nan

    y = np.log(r)
    X = np.vstack([np.ones_like(t), -t]).T
    beta, *_ = np.linalg.lstsq(X, y, rcond=None)
    lnR0_hat, lambda_hat = beta[0], beta[1]
    return float(np.exp(lnR0_hat)), float(lambda_hat)

ests = []
for (sid, tid), sub in interactions.groupby(["student_id","topic_id"]):
    R0_hat, lam_hat = estimate_lambda(sub)
    learned_on = sub["learned_on"].min()
    ests.append([sid, tid, R0_hat, lam_hat, learned_on])

recs = pd.DataFrame(ests, columns=["student_id","topic_id","R0_hat","lambda_hat","learned_on"])
recs.head()

# STEP 2.3 — Compute next review date

threshold = 0.60  # 60% retention

recs["t_threshold_days"] = -np.log(threshold / recs["R0_hat"]) / recs["lambda_hat"]

# Clean values: no negative days
recs["t_threshold_days"] = recs["t_threshold_days"].clip(lower=0)

# Compute the actual calendar date
recs["next_review_date"] = pd.to_datetime(recs["learned_on"]) + pd.to_timedelta(
    np.ceil(recs["t_threshold_days"]).astype(int), unit="D"
)

# Preview results
recs.head()

# STEP 2.4.1 — Save to CSV
recs.to_csv("recommendations.csv", index=False)
print("✅ Saved recommendations.csv with", len(recs), "rows")

# STEP 2.4.2 — Plot sample student–topic curves with threshold line

import matplotlib.pyplot as plt

sample_pairs = interactions.groupby(["student_id","topic_id"]).size().reset_index().sample(3)[["student_id","topic_id"]].values

plt.figure(figsize=(8,5))
for sid, tid in sample_pairs:
    sub = interactions[(interactions.student_id==sid) & (interactions.topic_id==tid)].sort_values("days_since_learned")
    plt.plot(sub["days_since_learned"], sub["score_observed"], marker="o", label=f"{sid}-{tid}")
plt.axhline(threshold*100, linestyle="--", color="red", label="Threshold 60%")
plt.xlabel("Days since learned"); plt.ylabel("Observed score (%)")
plt.title("Forgetting curves with review threshold")
plt.legend(); plt.show()