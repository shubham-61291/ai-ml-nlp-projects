# -*- coding: utf-8 -*-
"""MARS ‚Äì Memory and Retention Scheduling_using_keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tNrw6rtwJ5M-Wlh_m3npV3fb0enM_q99
"""

# STEP 1 ‚Äî Load data & schema check

import pandas as pd
import numpy as np

# Load your CSVs
students = pd.read_csv("students.csv")
topics = pd.read_csv("topics.csv")
interactions = pd.read_csv("interactions.csv")

# Ensure datetime parsing
for col in ["learned_on", "obs_date"]:
    if col in interactions.columns:
        interactions[col] = pd.to_datetime(interactions[col], errors="coerce")

# Define expected schema
exp_students = {"student_id", "grade", "ability", "diligence"}
exp_topics   = {"topic_id", "subject", "difficulty"}
exp_inter    = {"student_id","topic_id","learned_on","obs_date","days_since_learned","score_observed"}

missing = {
    "students.csv": list(exp_students - set(students.columns)),
    "topics.csv":   list(exp_topics   - set(topics.columns)),
    "interactions.csv": list(exp_inter - set(interactions.columns)),
}
problems = {k:v for k,v in missing.items() if v}
if problems:
    raise ValueError(f"Missing required columns: {problems}")

# Quick sanity checks
print("‚úÖ Loaded OK")
print("Shapes ‚Üí students:", students.shape, "| topics:", topics.shape, "| interactions:", interactions.shape)
print("Unique ‚Üí students:", interactions['student_id'].nunique(), "| topics:", interactions['topic_id'].nunique())
print("days_since_learned values:", sorted(interactions['days_since_learned'].unique())[:20])
display(students.head())
display(topics.head())
display(interactions.head())

# STEP 2 ‚Äî Merge data into ML dataset

# Merge students and topics into interactions
merged = interactions.merge(students, on="student_id", how="left") \
                     .merge(topics, on="topic_id", how="left")

print("‚úÖ Merged dataset shape:", merged.shape)
display(merged.head())

# --- Feature Engineering ---
# 1. Normalize observed scores (0‚Äì1)
merged["score_norm"] = merged["score_observed"] / 100.0

# 2. Safety: clip values
merged["score_norm"] = merged["score_norm"].clip(1e-4, 1.0)

# 3. Sort by time since learning
merged = merged.sort_values(["student_id","topic_id","days_since_learned"])

# At this stage, merged has:
# - Student features: grade, ability, diligence
# - Topic features: subject, difficulty
# - Interaction features: days_since_learned, score_norm
print("Columns available ‚Üí", merged.columns.tolist())

print(recs.columns)
recs.head()

# STEP 3 ‚Äî Prepare features (X) and target (y) from merged dataset

from sklearn.preprocessing import OneHotEncoder

# Make sure we are using merged dataset with subject_x
print("Columns available in merged dataset:", merged.columns.tolist())

# ‚úÖ Target
y = np.log1p(merged["lambda_true"].values)

# ‚úÖ Numeric features
X_num = merged[["grade_x","ability_x","diligence_x","difficulty_x"]].values

# ‚úÖ One-hot encode subject_x
enc = OneHotEncoder(sparse_output=False, handle_unknown="ignore")
X_cat = enc.fit_transform(merged[["subject_x"]])

# ‚úÖ Final feature matrix
X = np.hstack([X_num, X_cat])

print("‚úÖ Final dataset ready")
print("Features shape:", X.shape)
print("Target shape:", y.shape)
print("Sample y (log Œª):", y[:5])

# STEP 4 ‚Äî Train/Val/Test Split + Scaling

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# --- 80/10/10 split ---
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test   = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print("Train:", X_train.shape, "Val:", X_val.shape, "Test:", X_test.shape)

# --- Scale numeric features ---
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val   = scaler.transform(X_val)
X_test  = scaler.transform(X_test)

# --- Build Keras model ---
model = keras.Sequential([
    layers.Input(shape=(X_train.shape[1],)),
    layers.Dense(64, activation="relu"),
    layers.Dense(32, activation="relu"),
    layers.Dense(1)   # regression output (log Œª)
])

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss="mse",
    metrics=["mae"]
)

# --- Train ---
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    verbose=1
)

# STEP 5 ‚Äî Evaluation

# 1. Plot training curves
plt.figure(figsize=(8,5))
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.xlabel("Epochs"); plt.ylabel("MSE Loss")
plt.title("Training vs Validation Loss")
plt.legend(); plt.show()

# 2. Final evaluation on Test set
test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)
print(f"‚úÖ Test Loss (MSE): {test_loss:.4f}, Test MAE: {test_mae:.4f}")

# 3. Predictions (back-transform log Œª ‚Üí Œª)
y_pred = model.predict(X_test).flatten()
y_pred = np.expm1(y_pred)   # inverse of log1p
y_true = np.expm1(y_test)

# Compare few predictions
for i in range(5):
    print(f"True Œª: {y_true[i]:.4f} | Pred Œª: {y_pred[i]:.4f}")

# STEP 6 ‚Äî Generate next review recommendations from predicted Œª

# Threshold retention (same as baseline, 60%)
threshold = 0.60

# True Œª and Predicted Œª (already back-transformed)
lambda_true = y_true
lambda_pred = y_pred

# R0 (initial recall strength) use from recs or set to 1.0 baseline
R0 = 1.0

# --- Next review days calculation ---
# t_threshold = -ln(threshold / R0) / Œª
t_true = -np.log(threshold / R0) / lambda_true
t_pred = -np.log(threshold / R0) / lambda_pred

# --- Accuracy metrics ---
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

mae = mean_absolute_error(lambda_true, lambda_pred)
rmse = np.sqrt(mean_squared_error(lambda_true, lambda_pred))
r2   = r2_score(lambda_true, lambda_pred)

print("üìä Model Evaluation on Œª (Forgetting Rate)")
print(f"MAE:  {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R¬≤:   {r2:.4f}")

# --- Compare next review intervals ---
mae_days = mean_absolute_error(t_true, t_pred)
print(f"\nüìÖ MAE on Next Review Days: {mae_days:.2f} days")

# --- Show few sample rows ---
for i in range(5):
    print(f"True Œª={lambda_true[i]:.4f}, Pred Œª={lambda_pred[i]:.4f}, "
          f"True t*={t_true[i]:.1f}d, Pred t*={t_pred[i]:.1f}d")

# STEP 6 ‚Äî Generate next review recommendations + Plots

import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Threshold retention (same as baseline, 60%)
threshold = 0.60
R0 = 1.0  # assume baseline recall strength

# True & Predicted (already log1p inverted in Step 5)
lambda_true = y_true
lambda_pred = y_pred

# --- Next review days calculation ---
t_true = -np.log(threshold / R0) / lambda_true
t_pred = -np.log(threshold / R0) / lambda_pred

# --- Accuracy metrics ---
mae = mean_absolute_error(lambda_true, lambda_pred)
rmse = np.sqrt(mean_squared_error(lambda_true, lambda_pred))
r2   = r2_score(lambda_true, lambda_pred)
mae_days = mean_absolute_error(t_true, t_pred)

print("üìä Model Evaluation on Œª (Forgetting Rate)")
print(f"MAE:  {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R¬≤:   {r2:.4f}")
print(f"üìÖ MAE on Next Review Days: {mae_days:.2f} days")

# --- Sample comparisons ---
print("\nüîç Sample Predictions")
for i in range(5):
    print(f"True Œª={lambda_true[i]:.4f}, Pred Œª={lambda_pred[i]:.4f}, "
          f"True t*={t_true[i]:.1f}d, Pred t*={t_pred[i]:.1f}d")

# --- Plot 1: True vs Predicted Œª ---
plt.figure(figsize=(6,6))
plt.scatter(lambda_true, lambda_pred, alpha=0.4, label="Predictions")
plt.plot([lambda_true.min(), lambda_true.max()],
         [lambda_true.min(), lambda_true.max()],
         "r--", label="Ideal y=x")
plt.xlabel("True Œª")
plt.ylabel("Predicted Œª")
plt.title("True vs Predicted Forgetting Rate (Œª)")
plt.legend()
plt.show()

# --- Plot 2: Error distribution in review days ---
errors_days = t_pred - t_true
plt.figure(figsize=(8,5))
plt.hist(errors_days, bins=40, color="skyblue", edgecolor="black")
plt.axvline(0, color="red", linestyle="--")
plt.xlabel("Prediction Error in Next Review Days")
plt.ylabel("Frequency")
plt.title("Error Distribution in Predicted Review Schedule")
plt.show()

# STEP 7 ‚Äî Save Recommendations with Predicted Œª

import pandas as pd
import numpy as np

# Predicted Œª (positive scale)
lambda_pred_full = model.predict(scaler.transform(X)).flatten()
lambda_pred_full = np.expm1(lambda_pred_full)   # back-transform

# Compute next review days
threshold = 0.60
R0 = 1.0
t_pred_full = -np.log(threshold / R0) / lambda_pred_full
t_pred_full = np.clip(t_pred_full, 0, None)  # avoid negatives

# Add calendar next_review_date
merged["next_review_date_pred"] = pd.to_datetime(merged["learned_on"]) + pd.to_timedelta(
    np.ceil(t_pred_full).astype(int), unit="D"
)

# Add predicted Œª also
merged["lambda_pred"] = lambda_pred_full

# Select final columns for export
recommendations_ml = merged[[
    "student_id", "topic_id", "subject_x",
    "grade_x", "ability_x", "diligence_x", "difficulty_x",
    "lambda_true", "lambda_pred", "next_review_date_pred"
]]

# Save to CSV
recommendations_ml.to_csv("recommendations_ml.csv", index=False)
print("‚úÖ Saved recommendations_ml.csv with", len(recommendations_ml), "rows")

# Preview
recommendations_ml.head()