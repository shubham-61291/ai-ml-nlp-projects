# -*- coding: utf-8 -*-
"""sentiment analysis and churn rate prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jSncyqR428J6PgPVSsrX8Dan37StXvjG
"""

# STEP 1 ‚Äî Load dataset directly
import pandas as pd
import torch

# Check Torch + GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print("‚úÖ Torch version:", torch.__version__)
print("‚úÖ Using device:", device)

# Load CSV (already in Colab workspace)
nps_df = pd.read_csv("/content/nps_feedback.csv")

print("‚úÖ NPS Data Loaded | Shape:", nps_df.shape)
nps_df.head(10)

# STEP 2 ‚Äî Assign Sentiment Labels from NPS Score

def nps_to_sentiment(score):
    if score <= 6:
        return "Negative"
    elif score <= 8:
        return "Neutral"
    else:
        return "Positive"

# Apply function
nps_df["sentiment"] = nps_df["nps_score"].apply(nps_to_sentiment)

print("‚úÖ Sentiment labels assigned")
print(nps_df.head(10))

# Quick distribution check
print("\nSentiment distribution:")
print(nps_df["sentiment"].value_counts())

# STEP 3 ‚Äî Train/Val/Test Split + Baseline Sentiment Analysis

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# --- First split: Train (70%) + Temp (30%)
X_train, X_temp, y_train, y_temp = train_test_split(
    nps_df["feedback_text"], nps_df["sentiment"],
    test_size=0.3, random_state=42, stratify=nps_df["sentiment"]
)

# --- Second split: Validation (15%) + Test (15%)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.5, random_state=42, stratify=y_temp
)

print("‚úÖ Dataset sizes ‚Üí",
      "Train:", len(X_train),
      "Validation:", len(X_val),
      "Test:", len(X_test))

# --- TF-IDF vectorization
tfidf = TfidfVectorizer(stop_words="english", max_features=500)
X_train_tfidf = tfidf.fit_transform(X_train)
X_val_tfidf   = tfidf.transform(X_val)
X_test_tfidf  = tfidf.transform(X_test)

# --- Train model on Train + Validation tuning
clf = LogisticRegression(max_iter=500, random_state=42)
clf.fit(X_train_tfidf, y_train)

# --- Validation evaluation
y_val_pred = clf.predict(X_val_tfidf)
print("\nüìä Validation Report:")
print(classification_report(y_val, y_val_pred))

# --- Final Test evaluation
y_test_pred = clf.predict(X_test_tfidf)
print("\nüìä Test Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nüìä Test Report:")
print(classification_report(y_test, y_test_pred))

# --- Confusion Matrix on Test
cm = confusion_matrix(y_test, y_test_pred, labels=["Negative","Neutral","Positive"])
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Negative","Neutral","Positive"],
            yticklabels=["Negative","Neutral","Positive"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - Test Data")
plt.show()

!pip install -q transformers datasets accelerate scikit-learn evaluate

import pandas as pd
import numpy as np
import torch, random, os
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    DataCollatorWithPadding, Trainer, TrainingArguments
)
from datasets import Dataset, DatasetDict
import evaluate

# -------------------- 1) Reproducibility --------------------
seed = 42
random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("‚úÖ Using device:", device)

# -------------------- 2) Load Data --------------------
nps_df = pd.read_csv("nps_feedback.csv")

def map_sentiment(score):
    if score <= 6: return "Negative"
    elif score <= 8: return "Neutral"
    else: return "Positive"

nps_df["sentiment"] = nps_df["nps_score"].apply(map_sentiment)

# -------------------- 3) Train/Val/Test Split --------------------
X_train, X_temp, y_train, y_temp = train_test_split(
    nps_df["feedback_text"], nps_df["sentiment"],
    test_size=0.30, random_state=seed, stratify=nps_df["sentiment"]
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=seed, stratify=y_temp
)

print(f"Sizes ‚Üí Train:{len(X_train)} | Val:{len(X_val)} | Test:{len(X_test)}")

# -------------------- 4) Label Mapping --------------------
label2id = {"Negative":0, "Neutral":1, "Positive":2}
id2label = {v:k for k,v in label2id.items()}

def make_df(X, y):
    return pd.DataFrame({"text": X.values, "label": y.map(label2id).values})

train_df, val_df, test_df = make_df(X_train, y_train), make_df(X_val, y_val), make_df(X_test, y_test)

# -------------------- 5) Hugging Face Dataset Wrapper --------------------
ds = DatasetDict({
    "train": Dataset.from_pandas(train_df),
    "validation": Dataset.from_pandas(val_df),
    "test": Dataset.from_pandas(test_df)
})

# -------------------- 6) Tokenizer --------------------
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_fn(batch):
    return tokenizer(batch["text"], truncation=True, max_length=128)

ds_tok = ds.map(tokenize_fn, batched=True, remove_columns=["text"])
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# -------------------- 7) Model --------------------
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3,
    id2label=id2label,
    label2id=label2id
).to(device)

# -------------------- 8) Metrics --------------------
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="macro")["f1"]
    }

# -------------------- 9) Training Arguments --------------------
args = TrainingArguments(
    output_dir="./nps_distilbert",
    do_train=True,
    do_eval=False,                    # we‚Äôll evaluate manually
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    num_train_epochs=1,               # train 1 epoch at a time
    learning_rate=2e-5,
    seed=seed,
    fp16=torch.cuda.is_available(),
    report_to=[]                      # disable wandb/tensorboard
)

# -------------------- 10) Trainer --------------------
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=ds_tok["train"],
    eval_dataset=ds_tok["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# -------------------- 11) Manual Training Loop with Early Stopping --------------------
best_loss = float("inf")
patience, patience_counter = 2, 0

for epoch in range(10):  # max 10 epochs
    print(f"\nüöÄ Epoch {epoch+1}")
    trainer.train()

    # Evaluate on validation
    eval_result = trainer.evaluate(ds_tok["validation"])
    val_loss = eval_result["eval_loss"]
    print(f"Validation loss: {val_loss:.4f}")

    # Check improvement
    if val_loss < best_loss:
        best_loss = val_loss
        patience_counter = 0
        trainer.save_model("./best_model")
        print("‚úÖ Best model updated")
    else:
        patience_counter += 1
        print(f"‚ö†Ô∏è No improvement. Patience {patience_counter}/{patience}")
        if patience_counter >= patience:
            print("‚èπ Early stopping triggered")
            break

# -------------------- 12) Load Best Model --------------------
model = AutoModelForSequenceClassification.from_pretrained("./best_model").to(device)
trainer.model = model

# -------------------- 13) Evaluate on Test --------------------
test_out = trainer.predict(ds_tok["test"])
test_preds = np.argmax(test_out.predictions, axis=-1)
test_labels = test_out.label_ids

print("\nüìä Test Accuracy:", accuracy_score(test_labels, test_preds))
print("\nüìä Test Report:")
print(classification_report(test_labels, test_preds, target_names=[id2label[i] for i in range(3)]))

cm = confusion_matrix(test_labels, test_preds, labels=[0,1,2])
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=[id2label[i] for i in range(3)],
            yticklabels=[id2label[i] for i in range(3)])
plt.xlabel("Predicted"); plt.ylabel("True")
plt.title("DistilBERT ‚Äî Confusion Matrix (Test)")
plt.show()

# -------------------- 14) Inference Demo --------------------
examples = [
    "App is amazing and very helpful!",
    "It's okay, but sometimes confusing.",
    "Too many bugs, keeps crashing, not worth it."
]

tok = tokenizer(examples, return_tensors="pt", truncation=True, padding=True).to(device)
with torch.no_grad():
    logits = model(**tok).logits
probs = torch.softmax(logits, dim=-1).cpu().numpy()

for txt, p in zip(examples, probs):
    print(f"\nText: {txt}\nProbabilities ‚Üí Neg:{p[0]:.2f}  Neu:{p[1]:.2f}  Pos:{p[2]:.2f} | Pred: {id2label[int(p.argmax())]}")

# Track losses during training
train_losses = []
val_losses = []

best_loss = float("inf")
patience, patience_counter = 2, 0

for epoch in range(10):  # max 10 epochs
    print(f"\nüöÄ Epoch {epoch+1}")

    # Train for 1 epoch
    train_result = trainer.train()
    train_loss = train_result.training_loss
    train_losses.append(train_loss)

    # Evaluate on validation
    eval_result = trainer.evaluate(ds_tok["validation"])
    val_loss = eval_result["eval_loss"]
    val_losses.append(val_loss)
    print(f"Train loss: {train_loss:.4f} | Validation loss: {val_loss:.4f}")

    # Early stopping check
    if val_loss < best_loss:
        best_loss = val_loss
        patience_counter = 0
        trainer.save_model("./best_model")
        print("‚úÖ Best model updated")
    else:
        patience_counter += 1
        print(f"‚ö†Ô∏è No improvement. Patience {patience_counter}/{patience}")
        if patience_counter >= patience:
            print("‚èπ Early stopping triggered")
            break

# -------------------- Plot Loss Curve --------------------
plt.figure(figsize=(8,6))
plt.plot(train_losses, label="Train Loss", marker="o")
plt.plot(val_losses, label="Validation Loss", marker="o")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.show()

# NPS from your labelled data
promoters = (nps_df["nps_score"] >= 9).mean() * 100
detractors = (nps_df["nps_score"] <= 6).mean() * 100
nps_value = round(promoters - detractors, 2)

# Churn (Iteration) Rate ‚âà % of Detractors
churn_rate = (nps_df["nps_score"] <= 6).mean() * 100

# Retention Rate ‚âà % of Promoters + Passives
retention_rate = 100 - churn_rate

print(f"‚úÖ Estimated Iteration (Churn) Rate: {churn_rate:.2f}%")
print(f"‚úÖ Estimated Retention Rate: {retention_rate:.2f}%")
print(f"‚úÖ Net Promoter Score (NPS): {nps_value}")

