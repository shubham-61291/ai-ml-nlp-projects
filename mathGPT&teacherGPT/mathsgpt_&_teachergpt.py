# -*- coding: utf-8 -*-
"""MathsGPT_&_TeacherGPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kHhynx0QRGwwkAB8r4jor3WrODNUnevM
"""

# ========================================
# STEP 1 ‚Äî Load & Clean Dataset
# ========================================
import pandas as pd

csv_name = "mathgpt_dataset.csv"   # your uploaded dataset
df = pd.read_csv(csv_name)
print(f"\n‚úÖ Loaded: {csv_name} | shape={df.shape}")
print("Columns:", list(df.columns))

# normalize columns
df.columns = [c.strip().lower() for c in df.columns]
required = {"problem", "solution"}
missing = required - set(df.columns)
if missing:
    raise ValueError(f"CSV must have columns {required}. Missing: {missing}")

# keep only required
df = df[["problem", "solution"]].copy()

# clean
start_n = len(df)
df["problem"]  = df["problem"].astype(str).str.strip()
df["solution"] = df["solution"].astype(str).str.strip()
df = df.dropna(subset=["problem", "solution"])
df = df[(df["problem"] != "") & (df["solution"] != "")]
df = df.drop_duplicates(subset=["problem"]).reset_index(drop=True)
end_n = len(df)

print(f"\nüßπ Cleaned rows: kept {end_n}/{start_n} "
      f"(removed {start_n - end_n} empty/duplicate rows)")

# save cleaned dataset
clean_path = "mathgpt_dataset_clean.csv"
df.to_csv(clean_path, index=False)
print(f"\nüíæ Saved clean dataset ‚Üí {clean_path}")

# peek
print("\nüîé Sample rows:")
display(df.sample(min(5, len(df)), random_state=42))

# ========================================
# STEP 2 ‚Äî Dataset & DataLoader Prep
# ========================================
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import T5TokenizerFast

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("üî• Using device:", device)

# load FLAN-T5 tokenizer
tokenizer = T5TokenizerFast.from_pretrained("google/flan-t5-small")

# custom dataset
class MathDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_input_len=64, max_output_len=128):
        self.df = dataframe.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_input_len = max_input_len
        self.max_output_len = max_output_len

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        problem = f"Solve step by step: {str(self.df.loc[idx, 'problem'])}"
        solution = str(self.df.loc[idx, "solution"])

        inputs = self.tokenizer(
            problem,
            max_length=self.max_input_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        targets = self.tokenizer(
            solution,
            max_length=self.max_output_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        return {
            "input_ids": inputs["input_ids"].squeeze(),
            "attention_mask": inputs["attention_mask"].squeeze(),
            "labels": targets["input_ids"].squeeze()
        }

# build dataset
dataset = MathDataset(df, tokenizer)

# split 90/10
train_size = int(0.9 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=4)

print(f"‚úÖ Dataset size: {len(dataset)}")
print(f"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}")

batch = next(iter(train_loader))
print("Batch keys:", batch.keys())
print("Input IDs shape:", batch["input_ids"].shape)
print("Labels shape:", batch["labels"].shape)

# ========================================
# STEP 3 ‚Äî Training Loop (10 Epochs)
# ========================================
import torch.nn as nn
import torch.optim as optim
from transformers import T5ForConditionalGeneration

# load FLAN-T5 model
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-small").to(device)

optimizer = optim.AdamW(model.parameters(), lr=5e-5)
loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

epochs = 50
train_losses, val_losses = [], []

for epoch in range(epochs):
    # ---- train ----
    model.train()
    total_train_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_loader)

    # ---- val ----
    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_val_loss += loss.item()

    avg_val_loss = total_val_loss / len(val_loader)

    train_losses.append(avg_train_loss)
    val_losses.append(avg_val_loss)

    print(f"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

# save model
model.save_pretrained("mathgpt_flan_t5_small")
tokenizer.save_pretrained("mathgpt_flan_t5_small")
print("\n‚úÖ Training complete. Model saved ‚Üí 'mathgpt_flan_t5_small'")

# ========================================
# STEP 4 ‚Äî Plot + Prepare Inference (NO generation yet)
# ========================================
import matplotlib.pyplot as plt
from transformers import T5ForConditionalGeneration, T5TokenizerFast

# 4.1 ‚Äî Plot loss curves
plt.figure(figsize=(7,5))
plt.plot(train_losses, label="Train Loss", marker="o")
plt.plot(val_losses, label="Val Loss", marker="o")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss (FLAN-T5)")
plt.legend()
plt.grid(True)
plt.show()

# 4.2 ‚Äî Reload trained model/tokenizer
model_path = "mathgpt_flan_t5_small"
tokenizer = T5TokenizerFast.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)
model.eval()

# 4.3 ‚Äî Few-shot prompt + inference function (DO NOT CALL HERE)
FEW_SHOT_EXAMPLES = """
Problem: Solve 2x + 3 = 11
Solution:
Step 1: Subtract 3 from both sides ‚Üí 2x = 8.
Step 2: Divide both sides by 2 ‚Üí x = 4.

Problem: Compute 3/4 + 5/6
Solution:
Step 1: Find LCM of 4 and 6 ‚Üí 12.
Step 2: Convert ‚Üí 9/12 + 10/12.
Step 3: Add numerators ‚Üí 19/12.

Problem: Solve x^2 - 5x + 6 = 0
Solution:
Step 1: Factorize ‚Üí (x - 2)(x - 3) = 0.
Step 2: Roots ‚Üí x = 2 or x = 3.

Problem: Differentiate f(x) = 3x^2 + 2x - 5
Solution:
Step 1: Derivative of 3x^2 ‚Üí 6x.
Step 2: Derivative of 2x ‚Üí 2.
Step 3: Derivative of -5 ‚Üí 0.
Step 4: Final answer ‚Üí f'(x) = 6x + 2.

Problem: A coin is tossed twice. Find probability of at least one head.
Solution:
Step 1: Sample space = {HH, HT, TH, TT}.
Step 2: Favourable outcomes = {HH, HT, TH}.
Step 3: Probability = 3/4.
"""

def solve_math_problem(problem: str, max_new_tokens=250):
    """
    Returns a multi-step solution string.
    NOTE: This function is defined but NOT called in this cell.
    """
    prompt = FEW_SHOT_EXAMPLES + f"\nProblem: {problem}\nSolution (show all steps clearly):"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)

    output_ids = model.generate(
        **inputs,
        max_length=300,
        min_length=80,                  # force multiple steps
        num_beams=10,                   # explore more paths
        no_repeat_ngram_size=2,         # reduce loops
        repetition_penalty=1.4,
        temperature=0.7,
        early_stopping=False
    )
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# No inference calls below. Ready for next step.

# ========================================
# TeacherGPT ‚Äî Math Reasoning Assistant
# ========================================
import gradio as gr
from transformers import T5ForConditionalGeneration, T5TokenizerFast
import torch

# Load trained MathGPT (FLAN-T5 backbone)
model_path = "mathgpt_flan_t5_small"   # assume already trained & saved
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = T5TokenizerFast.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)
model.eval()

# Inference wrapper (no sample calls here)
def teacher_gpt(problem: str):
    prompt = f"Solve step by step: {problem}"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)

    output_ids = model.generate(
        **inputs,
        max_length=300,
        min_length=50,
        num_beams=5,
        no_repeat_ngram_size=2,
        repetition_penalty=1.3,
        temperature=0.7,
        early_stopping=False
    )
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# Gradio Frontend
with gr.Blocks() as demo:
    gr.Markdown("## üßë‚Äçüè´ TeacherGPT ‚Äî Math Reasoning Assistant")
    inp = gr.Textbox(lines=2, placeholder="Enter your math problem...")
    out = gr.Textbox(label="Step-by-Step Solution")
    btn = gr.Button("Get Solution")
    btn.click(fn=teacher_gpt, inputs=inp, outputs=out)

# Launch interface (not executed here)
# demo.launch()